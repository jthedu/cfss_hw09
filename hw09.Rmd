---
title: "HW09: Analyzing Daily Mail coverage on Meghan Markle"
author: "Julia Du"
date: "`r lubridate::today()`"
output: 
  github_document:
    toc: true
---

## Load necessary libraries

```{r, message = FALSE}
library(tidyverse)
library(stringr)
library(rvest)
library(glue)

library(tidytext)
library(reshape2)
library(wordcloud)
library(widyr)
library(ggraph)
library(igraph)

library(textrecipes)
library(topicmodels)

theme_set(theme_minimal())
set.seed(123)

```

## Overview

https://discuss.analyticsvidhya.com/t/how-to-remove-value-from-a-vector-in-r/2975/3

## Scraping & cleaning data

```{r mail}
# create folder to store local copies of df
dir.create("./data")

#mail <- read_html("https://www.dailymail.co.uk/tvshowbiz/meghan-markle/index.html")

mail_nodes <- html_nodes(mail, ".sport span , #content p, .linkro-ccow, .linkro-darkred a")
# use "p , .linkro-darkred a" to get w/o date

mail_vec <- mail_nodes %>%
  html_text()

mail_vec <- 
  mail_vec[c(-1, -400)] %>%
  append(c("", ""), after = 2) #adding spaces so that each article is divided into 5 elements

almost_mail <- mail_vec %>%
  split(ceiling(seq_along(mail_vec)/5)) %>%
  as_tibble() 

# stores local copy of dataframe
almost_mail %>%
  write_csv("./data/mail_210310.6pm.csv")

tidy_mail <- almost_mail %>%
  filter(!(. =="")) %>% # removing blank rows from all article columns
  # could use transpose here instead of pivot commands below (just t() & then as_tibble() in pipe)
  mutate(id = c("title", "date", "preview"), .before = 1) %>%
#  mutate(row = row_number()) %>%
  pivot_longer(-id, names_to = "recent_order", values_to = "values") %>%
  pivot_wider(names_from = id, values_from = values)

```

After starting my text analysis, it became clear that the 

## Looking at word frequency 

then can break the headline into tokens & look at sentiment. try and also find words correlated w/ Meghan Markle
then do same thing for preview

```{r}
#try to join title & preview columns
combined_mail <- tidy_mail %>%
  unite(col = combined_text, title, preview, sep = ". ")

# focusing on titles 
combined_word <- combined_mail %>%
  unnest_tokens(output = word, input = combined_text) %>%
  anti_join(stop_words) 

# word frequency
combined_word %>%
  count(word, sort = TRUE)
# As expected, we can see Oprah's interview with Meghan Markle & Prince Harry dominate the headlines about Markle. Now I'm going to remove these names to try getting a better idea of the content

common_words <- c("meghan's", "harry's", "winfrey", "interview", "markle")
#original list of common words: c("meghan", "markle", "meghan's", "harry's", "oprah", "interview")

custom_stop <- bind_rows(tibble(word = common_words, lexicon = c("custom")), stop_words)


combined_word %>%
  filter(!word %in% common_words) %>%
  count(word, sort = TRUE) %>%
  filter(n > 5)

# still not very informative
```

## Sentiment analysis

 https://www.tidytextmining.com/sentiment.html 

```{r}
bing_word <- combined_word %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) 

bing_word %>%
  group_by(sentiment) %>%
  slice_head(n = 5) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = "Analysis of Daily Mail titles & previews",
       subtitle = "Of articles on Meghan Markle", 
       x = "Contribution to sentiment",
       y = NULL)

combined_word %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("tomato", "cadetblue1"), 
                   max.words = 30, title.bg.colors = c("tomato", "cadetblue1")) 
# color reference: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf
  
```

## Pairwise correlation

drawn heavily from: https://www.tidytextmining.com/ngrams.html


```{r}
word_cors <- combined_word %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(item = word, feature = recent_order, sort = TRUE)
# looking at the common pairs of words co-appearing w/in each article (as defined by recent_order)

word_cors %>%
  filter(item1 %in% c("meghan", "harry")) %>%
  group_by(item1) %>%
  slice_head(n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(mapping = aes(x = item2, y = correlation)) +
  geom_col() +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

word_cors %>%
  filter(correlation > .1) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  ggtitle("Word pairs with at least a correlation of .1 appearing in the same article")
  theme_void()
```

not helpful. we're very limited in what we can do w/ just the title & 1st sent of an article, just not enough data to draw meaningful conclusions

## Topic modeling (Latent Dirichlet allocation)

https://cfss.uchicago.edu/notes/topic-modeling/

```{r}
custom_stop_vec <- custom_stop %>%
  pull(word)

mail_rec <- recipe(~., data = tidy_mail) %>%
  # even though already pre-processed data into combined_mail, 
  #redo tokenize/stopwords steps for tidy_mail so that R recognizes tokenized columns as tokenized
  step_tokenize(title, preview) %>%
  step_tokenmerge(title, preview, prefix = "combined_text") %>%
  step_stopwords(combined_text, custom_stopword_source = custom_stop_vec) %>%
# PROBLEM - CAN'T GET STOPWORD_SOURCE TO RECOGNIZE BOTH custom & regular stopwords
  # can do this: custom_stopword_source = custom_stop_vec

  step_ngram(combined_text, num_tokens = 1, min_num_tokens = 1) %>%
  step_tokenfilter(combined_text, max_tokens = 2500) %>%
  step_tf(combined_text)

test <- recipe(~., data = combined_word) %>%
  step_tokenize(word) %>% # THIS TELLS R THIS IS A TOKEN LIST
  step_ngram(word, num_tokens = 2, min_num_tokens = 1) %>%
  step_tokenfilter(word, max_tokens = 2500) %>%
  step_tf(word)

test_prep <- prep(test, verbose = TRUE)


mail_prep <- prep(mail_rec, verbose = TRUE)

mail_df <- bake(mail_prep, new_data = NULL) %>%
  select(-date) 
#drop date column. still have recent_order as an identifier for articles

#mail_df %>%
 # slice(1:5)

#convert to Document Term Matrix
mail_dtm <- mail_df %>%
  pivot_longer(cols = -c(recent_order),
               names_to = "token",
               values_to = "n") %>%
  filter(n != 0) %>%
  mutate(token = str_remove(string = token, pattern = "tf_combined_text_"),
         recent_order = fct_drop(f = recent_order)) %>%
  cast_dtm(document = recent_order, term = token, value = n)

mail_lda4 <- LDA(mail_dtm, k = 6, control = list(seed = 123))

top_terms_lda4 <- mail_lda4 %>%
  tidy() %>%
  group_by(topic) %>%
  slice_max(n = 5, order_by = beta, with_ties = FALSE) %>% #not returning all ties
  ungroup() %>%
  arrange(topic, -beta)

top_terms_lda4 %>%
  mutate(topic = factor(topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip() +
  theme(text = element_text(size = 5))

```

best results thus far (though that isn't exactly hard). in this case, trying to do topic classification also gives idea of general content. 


QUESTION - WHAT SHOULD I FILTER OUT BEFORE (I.E. MAKE STOPWORDS) & what should i keep?

think about filtering out articles w/ Piers Morgan?
we get "fearless" from ppl praising Morgan (& implicitly supporting his derogatory comments towards Markle)







do something similar w/ Reddit jokes & see which words pop up most frequently, try to find the most common topics
or also the correlated words -- think Hamilton? or Trump tweet examples


```{r}
vec_3.12_2pm <- read_html("https://www.dailymail.co.uk/tvshowbiz/meghan-markle/index.html") %>%
  html_nodes(".sport span , #content p, .linkro-ccow, .linkro-darkred a") %>%
  html_text() 

vec_3.12_2pm <-
  vec_3.12_2pm[c(-1, -400)] %>%
  append(c("", ""), after = 2) 

data_3.12_2pm <- vec_3.12_2pm %>%
  split(ceiling(seq_along(vec_3.12_2pm)/5)) %>%
  as_tibble()

data_3.12_2pm %>%
  write_csv("./data/mail_210312.2pm.csv")

```

## Session info

```{r, echo = TRUE}
devtools::session_info()
```